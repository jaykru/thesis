% Created 2021-09-13 Mon 08:51
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Jay Kruer}
\date{\today}
\title{9/14: Up to Speed on Stirling \& Spitters}
\hypersetup{
 pdfauthor={Jay Kruer},
 pdftitle={9/14: Up to Speed on Stirling \& Spitters},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.5)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Reminders of some notions from my first chapter}
\label{sec:orgf58096a}

\section{Two Yoneda embeddings and normalization by evaluation}
\label{sec:org37b24cb}
\subsection{Normalization by evaluation}
\label{sec:org9943bee}
Normalization by evaluation is a technique used to demonstrate normalization--
the property that all terms have a normal form--for some lambda calculus.
Andreas Abel renders the technique very clearly\footnote{\url{https://www.cse.chalmers.se/\~abela/talkEAFIT2017.pdf}}: Normalization is the
process of bringing an open term (with unknowns) to a special kind of fixed
point. A similar notion is evaluation, the process of bringing a \emph{closure},
namely an open term paired with a substitution closing it, to a canonical form
comprised entirely of constructors.

\textbf{Normalization by evaluation} borrows an exist evaluator for some sufficiently
expressive host language and uses it to normalize an open expression in the
guest language we're interested in. The basic plot outline is this, each step
paired with the relevant code from an instance\footnote{\url{https://en.wikipedia.org/wiki/Normalisation\_by\_evaluation}} of normalization with the host
being Standard ML and the guest being the simply typed lambda calculus:
\begin{enumerate}
\item Create in the host language an internal representation of the guest language syntax and type structure:
\begin{verbatim}
    datatype ty = Basic of string
                | Arrow of ty * ty
                | Prod of ty * ty


\end{verbatim}
\item 
\end{enumerate}



This is accomplished
through a mutually recursive pair of operations called reification and
reflection.
\subsection{Reification, reflection}
\label{sec:org43081b6}


\subsection{Just work from wikipedia for this part}
\label{sec:org3258be1}

\subsection{The readback used in StirSpit}
\label{sec:org7b3c6f8}
Note that I haven't yet seen exactly the readback operation (in terms of
presheaves) planned yet. Their presentation is abysmally bad on this point
actually\ldots{}they delay explaining/defining an object of deep conceptual and
technical import to the result until like 10 pages in. Maybe they suppose that
the reader already has experience with normalization by evaluation, which I
guess is kind of fair, but this was personally my first exposure to it in
earnest.

\section{A pickle: too much quotienting}
\label{sec:orgc2f5300}
The definition of the syntactic category (category of contexts and
substitutions) given in the chapter I wrote a few weeks ago has for its
equations governing equality of morphisms those given by the ``substitution
lemma.'' It turns out that this identifies far too many terms for our uses. In
particular, terms which are related by the various beta rules are identified,
meaning that a normal form (a term for which no further beta reduction can be
performed) is identified with its (manifestly not normal) beta-predecessor. The
upshot is that we can't isolate the normal forms as a class of terms, which
totally bungles our whole project of investigating which terms (all of them) of
the simply typed lambda calculus have normal forms.

\section{Unpickling ourselves: the category of renamings}
\label{sec:org6815046}

\section{The relative hom functor}
\label{sec:orgefff5a7}
Stirling \& Spitters follow Fiore in defining the ``relative hom functor'', which
they suggestively call \(\mathfrak{Tm} : \text{Cl}_{\Sigma} \rightarrow
{\text{Ren}_\Sigma}^{\textsc{Set}}\). The suggestion hinted at by the name,
that this functor defines a presheaf of open terms, turns out to be a (small?)
lie. Let's look at what it actually does. \(\mathfrak{Tm}\) is defined by
adjusting the hom functor (i.e, the Yoneda embedding) by precomposition with the
inclusion of the category of renamings into the category of (contexts and)
substitutions. In particular, StirSpit define \(\mathfrak{Tm}(\Delta) =
\text{Cl}_{\Sigma} [i(-), \Delta]\). In plain terms, \(\mathfrak{Tm}(\Delta)\)
takes a context in the category of renamings to the \emph{substitutions on terms}
\textbf{out of} \(\Delta\) (recalling that the action of the category of contexts on
its clones is contravariant). This can be (very loosely) construed as a presheaf
of open terms. For a (renaming) context \(\Gamma\), we have
\(\mathfrak{Tm}(\Delta)(\Gamma) = \text{Cl}_{\Sigma} [i(\Gamma), \Delta]\), the
context \(\Gamma\) just falls through and we get the substitutions \(\gamma^{*} : \Delta \vdash \tau \rightarrow \Gamma \vdash \tau\) for arbitrary
\(\tau\). In particular, any (possibly) open term \(\Delta \vdash t : \tau\)
is included in \(\mathfrak{Tm}(\Delta,\tau)(\Delta)\) as the single
substitution \([t/x]\). The reason I regard as misleading the suggestion that
the relative hom functor defines a presheaf of open terms is that the morphisms
in the syntactic category aren't just single substitutions, but also single
omissions \(\hat{x}\) and all the compositions of these two classes of maps.
\end{document}
