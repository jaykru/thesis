#+TITLE: 9/14: Up to Speed on Sterling & Spitters --- modified notes for later inclusion in thesis
* Full implementation of NBE for Godel's System T
#+begin_src ocaml
open Core

type ty = Arrow of ty * ty
        | Prod of ty * ty
        | Nat [@@deriving sexp]

type var = string [@@deriving sexp]

let cur : int ref = ref 0

let fresh () : var = let res = Printf.sprintf "var%d" !cur in
                     cur := !cur + 1;
                     res

type syn = Lam of var * ty * syn
         | App of syn * syn

         | Pair of syn * syn
         | P1 of syn
         | P2 of syn

         | Zero
         | Succ of syn
         | Rec of ty * syn * syn * syn (* rec : forall (T: ty), T -> (N -> T -> T) -> N -> T *)

         | Var of var [@@deriving sexp]

(* Note: There are no elimination forms in the semantics; they are represented
   as syntax through the SYN embedding. *)
type sem = LAM of (sem -> sem)
         | PAIR of sem * sem
         | SYN of syn
[@@deriving sexp]

(* reflect is a type-indexed family of functions taking the syntax into the
   semantics *)
let rec reflect (tau : ty) : syn -> sem =
  fun t ->
  match tau with
  | Arrow (a, b) ->
    LAM (fun s -> reflect b (App (t, (reify a s)) ))
  | Prod (a, b) ->
    PAIR ((reflect a (P1 t), (reflect b (P1 t))))
  | Nat ->
    SYN t

and reify (tau : ty) : sem -> syn =
  fun t ->
  match (t, tau) with
  | (LAM s, Arrow (a, b)) ->
    let x = fresh () in
    Lam (x, tau, reify b (s (reflect a (Var x))))
  | (PAIR (l, r), Prod(a,b)) ->
    Pair (reify a l, reify b r)
  | (SYN b, _) ->
    b
  | _ ->
    raise (Failure (Printf.sprintf "Reification failed on (%s : %s)"
                      ([%sexp_of : sem] t |> Sexp.to_string)
                      ([%sexp_of: ty] tau |> Sexp.to_string)))


let rec metarec (tau : ty) (z : 'a) (s : syn -> 'a -> 'a) : syn -> 'a =
  fun n ->
  match n with
  | Zero -> z
  | Succ n ->
    s n (metarec tau z s n)
  | u ->
    let v_z = reify tau z in
    let v_s = reify (Arrow(Nat, (Arrow (tau, tau)))) (LAM (fun n -> match n with
                                                            | SYN n -> LAM(s n)
                                                            | _ -> raise (Failure "in metarec"))) in
    reflect tau (Rec (tau, v_z, v_s, u))

type context = (var * ty) list

(* A valuation is a list semantic terms to be interpreted as an assignment of
   terms to variables in an ordered context *)
type valuation = (var * sem) list


(* we assume that a bad lookup is never performed; this will end up being the
   case as long as we only take the meaning in the valuation generated by the
   context closing a well-typed term. *)
let lookup (x : var) (rho : valuation) : sem =
  List.rev (List.filter ~f:(fun entry -> String.equal (fst entry) x) rho)
  |> fun l -> List.nth_exn l 0
  |> snd

(* Given a context gamma, produces a valuation of reflected variables *)
let reflect_ctx (gamma : context) : valuation =
  List.map ~f:(fun (x,tau) -> (x, reflect tau (Var x))) gamma

(* interprets into the semantics an open term t with respect to a valuation rho
   which closes it *)
let rec interp (t : syn) (rho : valuation): sem =
  match t with
  | Var x ->
    lookup x rho
  | Lam(x, _, b) ->
    LAM (fun s -> interp b (List.append rho [(x,s)]))
  | App(e1, e2) ->
    let LAM e1f = interp e1 rho in
    e1f (interp e2 rho)
  | Pair(e1, e2) ->
    PAIR(interp e1 rho, interp e2 rho)
  | P1 e ->

    let PAIR (l,_) = interp e rho in
    l
  | P2 e ->
    let PAIR (_,r) = interp e rho in
    r
  | Zero -> SYN Zero
  | Succ n ->
    SYN (Succ n)
  | Rec(tau, z, s, n) ->
    let LAM si = interp s rho in (* s : nat -> A -> A *)
    let zi = interp z rho in
    let SYN ni = interp n rho in
    metarec tau zi (fun m -> let LAM(f) = si (SYN m) in f) ni
    [@@warning "-8"]
(* the above incomplete matches are irrefutable on well-typed terms t *)

let nbe (gamma : context) (tau : ty) : syn -> syn =
  fun t -> interp t (reflect_ctx gamma) |> reify tau

(* EXAMPLES:
    nbe [("y", Nat); ("z", Nat)] Nat (App (Lam("x", Nat, Var("y")), Var ("z")));;
    - : syn = Var "y"

   nbe [] Nat (App ((Lam ("z", Nat, (Rec(Nat, Zero, Lam("x",Nat,Lam("y",Nat,(Var "x"))), (Var "z"))))), (Succ Zero)));;
   - : syn = Zero*)
#+end_src

* Reminders of some notions from my first chapter
Review the category of contexts and substitutions, I can do this live.
* The Yoneda embedding and normalization by evaluation
** Normalization? Evaluation?
Normalization by evaluation is a technique used to demonstrate normalization--
the property that all terms have a normal form--for some lambda calculus.
Andreas Abel renders the technique very clearly[fn:1]: Normalization is the
process of bringing an open term (with unknowns) to a special kind of fixed
point. A similar notion is evaluation, the process of bringing a /closure/,
namely an open term paired with a substitution closing it, to a canonical form
comprised entirely of constructors.

*Normalization by evaluation* borrows an exist evaluator for some sufficiently
expressive host language and uses it to normalize an open expression in the
guest language we're interested in. The basic plot outline is this, each step
paired with the relevant code from an instance[fn:2] of normalization with the host
being Standard ML and the guest being the simply typed lambda calculus:
1. Create in the host language an internal representation of the guest language
   syntax and type structure:
   #+begin_src sml
    datatype ty = Basic of string
                | Arrow of ty * ty
                | Prod of ty * ty

    datatype tm = var of string
                | lam of string * tm | app of tm * tm
                | pair of tm * tm | fst of tm | snd of tm
   #+end_src
2. Building a semantic model (a la actions) of the guest language in terms of host language constructs:

   #+begin_src sml
   datatype sem = LAM of (sem -> sem)
                | PAIR of sem * sem
                | SYN of tm
   #+end_src

      Here, our model expresses lambda expressions in STLC as /actual ML
   functions/ and pairs as /actual ML pairs/. We also allow for embedding syntax
   terms of our base type, which have no further structure, into the semantics.

3. Defining a mutually recursive pair of type-indexed operation families called
   reification and reflection, detailed in the next section, whose composition
   acts as a normalization function.
4. Compose reification with reflection to get a normalization function for terms
   in the guest language.

** Reflection, reification
[[file:nbe.png]]
Reflection is the operation which takes a syntactic lambda term, namely
something in ~tm~, and produces the corresponding semantic representation in
~sem~. The instance from Wikipedia features the following reflection operation
defined mutually with reification:
#+begin_src sml
 (* reflect : ty -> tm -> sem *)
 fun reflect (Arrow (a, b)) t =
       LAM (fn S => reflect b (app t (reify a S)))
   | reflect (Prod (a, b)) t =
       PAIR (reflect a (fst t)) (reflect b (snd t))
   | reflect (Basic _) t =
       SYN t

(* reify : ty -> sem -> tm *)
 and reify (Arrow (a, b)) (LAM S) =
       let x = fresh_var () in
         lam (x, reify b (S (reflect a (var x))))
       end
   | reify (Prod (a, b)) (PAIR S T) =
       pair (reify a S, reify b T)
   | reify (Basic _) (SYN t) = t
#+end_src

I'll start my explanation of this with reification, because it will come first
in the normalization function we're building. In its first case, we want to
reflect a syntactic function (something of arrow type) into our semantics. To do
so, we create a new ML function (with a lambda expression) which reflects into
the semantics the result of applying the syntactic function to the reification
of its parameter. This resembles the usual eta-expansion for terms of function
type, in which we take \( f: A \rightarrow B\) to the term \( \lambda a.\, f a
\). The pair case has a similarly eta-expansive flavor. The case for the base
type just embeds the term into the semantics.

Reification is just like reflection but with the parity of
reflection/reification calls flipped everywhere. As before, for function types
we perform something like eta-expansion; only here we can't rely on the
meta-language to handle variable freshness issues and we have to allocate
hitherto-unbound variables ourselves. Pairs are similar, without the variable
complication. The case of the base type amounts to undoing the embedding
performed by reflect.

One can show that the composition ~fun t. reify (reflect (t))~ is a normalization
function for the simply typed lambda calculus.

** Back to SterSpit
The first big claim in SterSpit is that a special version of the Yoneda
embedding (with more intensional flavor) defines a reflection operation from the
syntactic category into its category of presheaves. SterSpit promise a
corresponding reification operation to complete the normalization by evaluation
story, but delay its introduction until a few sections past what I've managed to
understand so far. For now, we have a problem.

* A pickle: too much quotienting
The definition of the syntactic category (category of contexts and
substitutions) given in the chapter I wrote a few weeks ago has for its
equations governing equality of morphisms those given by the "substitution
lemma." It turns out that this identifies far too many terms for our uses. In
particular, terms which are related by the various beta rules are identified,
meaning that a normal form (a term for which no further beta reduction can be
performed) is identified with its (manifestly not normal) beta-predecessor. The
upshot is that we can't isolate the normal forms as a class of terms, which
totally bungles our whole project of investigating which terms (all of them) of
the simply typed lambda calculus have normal forms. To spell it out: the
normalization by evaluation function SterSpit propose is actually equal to the
identity on the syntactic category. The whole point of using the gluing
construction is to get us out of this situation by introducing more
computational information that allows us to distinguish terms from their
beta-ancestors and beta-posteriors.

* Unpickling ourselves: the category of renamings
As an alternative to the misbehaving category of contexts and substitutions, we
will work with the catetory of renamings. In simple terms, this category has as
its objects all contexts over our types and whose morphisms are type-preserving
changes of variable from one context to another. Unfortunately as I found while
typing this up, the simple rendition is somewhat ill-specified. Fiore (2002)
defines this category as the opposite of the comma category \( \mathfrak{i}
\downarrow \mathfrak{T} \) where \( \mathfrak{i} \) is the functor embedding the
category \( \mathbb{F} \) of finite sets of variables (drawn from the supply of
our algebraic theory) into \( \mathfrak{Set} \) and \( \mathfrak{T} \) is the
constant functor for our set of types \( \mathfrak{T} \). Concretely, its
objects are maps \( \Gamma : V \rightarrow \mathfrak{T} \), i.e., exactly
contexts assigning types to finitely many variables. The morphisms \( \Gamma'
\rightarrow \Gamma \) (of the opposite we'll be working with) are functions \(
\rho : \text{dom}(\Gamma) \rightarrow \text{dom}(\Gamma') \) such that \(\Gamma
= \Gamma' \circ \rho \), i.e., the morphisms are type-preserving changes of
variables. The reason we work with the opposite of this category is so that the
action of the category of the renamings acts contravariantly on the clones of
the syntactic category it embeds in; i.e., we want the arrows to line up with
the syntactic category.

* The relative hom functor
Sterling & Spitters follow Fiore in defining the "relative hom functor", which
they suggestively call \(\mathfrak{Tm} : \text{Cl}_{\Sigma} \rightarrow
{\text{Ren}_\Sigma}^{\mathfrak{Set}} \). The suggestion hinted at by the name,
that this functor defines a presheaf of open terms, turns out to be a small lie.
Let's look at what it actually does. \(\mathfrak{Tm}\) is defined by adjusting
the hom functor (i.e, the Yoneda embedding) by precomposition with the inclusion
of the category of renamings into the category of (contexts and) substitutions.
In particular, SterSpit define \(\mathfrak{Tm}(\Delta) = \text{Cl}_{\Sigma}
[i(-), \Delta]\). In plain terms, \( \mathfrak{Tm}(\Delta)\) takes a context in
the category of renamings to the /substitutions on terms/ *out of* \(\Delta\)
(recalling that the action of the category of contexts on its clones is
contravariant). This can be construed as a presheaf of open terms. For any type
\( \tau \) and any context \( \Gamma \), the morphisms in this set are (in their
totality):
1. Single substitutions \( [t / x] \) where \( \Gamma \vdash t : \tau \). (Note
   that this includes renamings); and,
2. Context weakenings \( \hat{y} \)

Ignoring the weakenings, we see that we have a presheaf of open terms closed by
\( \Gamma \).

The suggestion does break down when generalizing the target context from a
single \( \tau \). For contexts \( \Delta \) and \( \Gamma \), we have
\(\mathfrak{Tm}(\Delta)(\Gamma) = \text{Cl}_{\Sigma} [i(\Gamma), \Delta]\), the
context \( \Gamma \) just falls through the inclusion and we get the
substitutions \( \gamma^{*} : \Delta \vdash \tau \rightarrow \Gamma \vdash \tau
\) for arbitrary \( \tau \). Now here comes the (small) lie: the morphisms in
the syntactic category aren't just single substitutions, but also (as before)
single omissions \( \hat{x}\) and all the compositions of these two classes of
maps.

* Footnotes
[fn:2] https://en.wikipedia.org/wiki/Normalisation_by_evaluation
[fn:1] https://www.cse.chalmers.se/~abela/talkEAFIT2017.pdf
